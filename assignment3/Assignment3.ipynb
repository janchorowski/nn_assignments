{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/nn_assignments/blob/nn18/assignment3/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LpymfDg-Qe9h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Assignment 3\n",
        "\n",
        "**Submission deadline: last lab session before or on Friday, 16.11.17**\n",
        "\n",
        "**Points: 12 + 7 bonus points**\n"
      ]
    },
    {
      "metadata": {
        "id": "WG40OfXGUb42",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please note that this code needs only to be run in a fresh runtime.\n",
        "# However, it can be rerun afterwards too.\n",
        "!pip install -q gdown httpimport\n",
        "![ -e mnist.npz ] || gdown 'https://drive.google.com/uc?id=1QPaC3IKB_5tX6yIZgRgkpcqFrfVqPTXU' -O mnist.npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "npdbTitzQe9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%pylab inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8x_leeOUpdl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import httpimport\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('whitegrid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ow-GpKTRQe-O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Gradient Descent [2p + 2b]\n",
        "\n",
        "The Gradient Descent (GD) algorithm finds the minimum of a given\n",
        "function by taking small steps along the function's gradient. In\n",
        "pseudocode:\n",
        "\n",
        ">$\\Theta \\leftarrow \\Theta_0$\n",
        "\n",
        ">**while** stop condition not met **do**\n",
        "\n",
        ">$~~~~$$\\Theta \\leftarrow \\Theta - \\alpha \\nabla_\\Theta f(\\Theta)$\n",
        "\n",
        ">**end while**\n",
        "\n",
        "where $f$ is the function to minimize, $\\nabla_\\Theta f(\\Theta)$\n",
        "denotes $f$'s gradient at $\\Theta$ and $\\alpha$ is the step size,\n",
        "taking typically values from $10^{-4},\\ldots,10^{-1}$.\n",
        "\n",
        "\n",
        "1. **[1p]** Implement the GD algorithm as a function:\n",
        "\n",
        "  \\begin{equation}\n",
        "      \\Theta_{opt} = \\text{GD}(f, \\Theta_0, \\alpha, \\rho),\n",
        "  \\end{equation}\n",
        "\n",
        "  where $f$ is a function returning the cost and the gradient of the\n",
        "  cost with respect to parameter vector\n",
        "  $\\Theta$, $\\Theta_0$ is the initial value, and $\\alpha$\n",
        "  is the step size (a.k.a. the learning rate).\n",
        "  You can assume that $\\alpha$\n",
        "  remains constant throughout the optimization.\n",
        "  Terminate when the function values will differ by less than $\\rho$\n",
        "  between subsequent iterations, eg. by $10^{-10}$.\n",
        "\n",
        "2. **[1p]** Use the GD algorithm to find the optimum of the\n",
        "  Rosenbrock (https://en.wikipedia.org/wiki/Rosenbrock_function) function.\n",
        "  Set $(0,2)$ as the initial point. Try to set an appropriate learning rate\n",
        "  $\\alpha$.\n",
        "\n",
        "  Plot the values found by GD at subsequent iterations. Set log scale for\n",
        "  the Y axis.\n",
        "\n",
        "  Plot function contours and values of $\\Theta$ at subsequent\n",
        "  iterations.\n",
        "  \n",
        "  **Note**: You can debug your implementation by using the\n",
        "  gradient checking routines.\n",
        "  \n",
        "  Numerical optimization is of great importance,\n",
        "  and many algorithms beside GD exists. Get familiar with the L-BFGS\n",
        "  algorithm\n",
        "  (for Python: `scipy.optimize.fmin_l_bfgs_b`). Use the\n",
        "  L-BFGS algorithm to find the optimum of the Rosenbrock function\n",
        "  and plot the contours and $\\Theta$'s in subsequent iterations.\n",
        "\n",
        "  How many iterations do BGD and L-BFGS need to find a point, for\n",
        "  which the Rosenbrock function value is lower than $10^{-10}$?\n",
        "  \n",
        "3. **[2p bonus]** Implement Newton's method (https://en.wikipedia.org/wiki/Newton's_method_in_optimization) and compare it with the previous methods. You will also need to implement a line search alogithm, e.g. (https://en.wikipedia.org/wiki/Backtracking_line_search) and make sure that the Newton's direction is indeed one along which the function is minimized (the Newton method is only guaranteed to work on convex functions, and behaves badly near critical points of non-convex functions)."
      ]
    },
    {
      "metadata": {
        "id": "MMKiSdNZQe-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Implement the Rosenbrock function\n",
        "#\n",
        "\n",
        "with httpimport.github_repo('janchorowski', 'nn_assignments', \n",
        "                            module='common', branch='nn18'):\n",
        "    from common.gradients import check_gradient\n",
        "\n",
        "def rosenbrock_v(x):\n",
        "    \"\"\"Returns the value of Rosenbrock's function at x\"\"\"\n",
        "    return TOOD\n",
        "\n",
        "def rosenbrock(x):\n",
        "    \"\"\"Returns the value of rosenbrock's function and its gradient at x\n",
        "    \"\"\"\n",
        "    val = TODO\n",
        "    dVdX= TODO\n",
        "    return [val, dVdX]\n",
        "\n",
        "#\n",
        "# Feel free to add your own test points.\n",
        "#\n",
        "for test_point in [[0., 0.],\n",
        "                   [1., 1.],\n",
        "                   [0.5, 1.0],\n",
        "                   [1.0, 0.5]]:\n",
        "    assert check_gradient(rosenbrock, np.array(test_point), prec=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aymI0gTxQe-b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Fill in the details of the gradient descent\n",
        "#\n",
        "\n",
        "def GD(f, Theta0, alpha, stop_tolerance=1e-10, max_steps=1000000):\n",
        "    \"\"\"Runs gradient descent algorithm on f.\n",
        "    \n",
        "    The baisic iteration is:\n",
        "    val, dVdTheta <- f(Theta)\n",
        "    Theta <- -alpha * dVdTheta\n",
        "    \n",
        "    Args:\n",
        "        f: function that when evalueted on a Theta of same dtype and shape as Theta0\n",
        "            returns a tuple (value, dVdTheta) with dValuedTheta of the same shape\n",
        "            as Theta\n",
        "        Theta0: starting point\n",
        "        alpha: step length\n",
        "        stop_tolerance: stop iterations when improvement is below this threhsold\n",
        "        max_steps: maximum number of steps\n",
        "        \n",
        "    Returns:\n",
        "        tuple:\n",
        "        - ThetaOpt\n",
        "        - history: list of length num_steps containing tuples (Theta, (val, dValdTheta))\n",
        "    \n",
        "    \"\"\"\n",
        "    history = []\n",
        "    \n",
        "    Theta = Theta0\n",
        "    value = np.inf\n",
        "    \n",
        "    step = 0\n",
        "    while step < max_steps:\n",
        "        previous_value = value\n",
        "        value, gradient = f(Theta)\n",
        "        history.append([Theta, (value, gradient)])\n",
        "        \n",
        "        if TODO < stop_tolerance:\n",
        "            break\n",
        "        \n",
        "        Theta = TODO\n",
        "        \n",
        "        history.append([Theta, f(Theta)])\n",
        "        step += 1\n",
        "    \n",
        "    history.append([Theta, f(Theta)])\n",
        "    return Theta, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D49vfq6JQe-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Find the optimum\n",
        "#\n",
        "\n",
        "X0 = [0.,2.]\n",
        "Xopt, Xhist = GD(rosenbrock, X0, alpha=1e-3, stop_tolerance=1e-10, max_steps=1e6)\n",
        "\n",
        "print \"Found optimum at %s in %d steps (true minimum is at [1,1])\" % (Xopt, len(Xhist))\n",
        "\n",
        "#\n",
        "# Plot how the value changes over iterations\n",
        "#\n",
        "\n",
        "TODO\n",
        "\n",
        "#\n",
        "# Make a contour plot of the Rosenbrock function and show the trajectory\n",
        "# used by gradient descent.\n",
        "#\n",
        "# you may want to use functions: meshgrid, contour\n",
        "#\n",
        "\n",
        "TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NVBvwnWlQe-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Use scipy.optimize.fmin_l_bfgs_b\n",
        "# Again plot the path on the coutnour plot\n",
        "#\n",
        "# Hint: to save the points you can use the callback argument!\n",
        "#\n",
        "\n",
        "import scipy.optimize as sopt\n",
        "lbfsg_hist = []\n",
        "def save_hist(x):\n",
        "    lbfsg_hist.append(np.array(x))\n",
        "\n",
        "x_start = [0.,2.]\n",
        "lbfsgb_ret = sopt.fmin_l_bfgs_b(TODO, callback=save_hist)\n",
        "\n",
        "#\n",
        "# TODO: make a conour plot, show points considered by l_bfsgb algorithm and by gradient descent.\n",
        "# How many steps did l-bfgs take to optimize the Rosenbrock function?\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oR8cwMrOQe-4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Bonus problem\n",
        "#\n",
        "\n",
        "# Newtod-Raphson Method\n",
        "\n",
        "def rosenbrock_hessian(x):\n",
        "    \n",
        "    # TODO: compute the value, gradient and Hessian of Rosenbrock's function'\n",
        "    \n",
        "    return [val, np.array((dvdx0, dvdx1)), H]\n",
        "\n",
        "\n",
        "def Newton(f, Theta0, alpha, stop_tolerance=1e-10, max_steps=1000000):\n",
        "    \n",
        "    # TODO:\n",
        "    #  - implement the newton method and a simple line search\n",
        "    #  - make sure your function is resilient at critical points (such as seddle points)\n",
        "    #  - if the Newton direction is not minimizing the function, use the gradient for a few steps\n",
        "    #  - try to beat L-BFGS on the bmber of function evaluations needed!\n",
        "    \n",
        "    raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ck2xJ_EQe_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Logistic Regression [1p]\n",
        "\n",
        "Linear regression is suitable for problems, where\n",
        "the forecasted values are real numbers. We use logistic regression,\n",
        "when we want to label the data with $0$ and $1$.\n",
        "\n",
        "Let $x\\in \\mathbb{R}^n$ be a vector of $n$ real numbers, and\n",
        "$y\\in \\{0,1\\}$ the given class label. Similarly to what was shown\n",
        "during the lecture, we add an additional element $x_0=1$\n",
        "to vector $x$, to account for the bias term (and simplify the equations).\n",
        "\n",
        "Similarly to linear regression, vector\n",
        "$\\Theta\\in \\mathbb{R}^{n+1}$ parametrizes the model\n",
        "($n$ coefficients describes the data, the remaining one is the intercept).\n",
        "In logistic regression, we model conditional probability that\n",
        "sample $x$ belongs to class $1$ as:\n",
        "\n",
        "\\begin{equation}\n",
        "p(\\text{class}=1|x, \\Theta)=h_\\Theta(x) = \\sigma\\left(\\sum_{j=0}^n \\Theta_j x_j\\right) \n",
        "= \\sigma\\left(\\Theta^T x \\right),\n",
        "\\end{equation}\n",
        "\n",
        "where $\\sigma(a) = \\frac{1}{1+\\exp(-a)}$ is being called the logistic sigmoid\n",
        "(a function, which plot is s-curved).\n",
        "\n",
        "An unknown sample $x$ is being labeled $1$ if\n",
        "$h_\\Theta(x)\\geq 0.5$, or equivalently, $\\Theta^T x \\geq 0$.\n",
        "\n",
        "Classification mismatch between the forecasted values and\n",
        "the data is being measured most of the time with cross-entropy:\n",
        "\n",
        "\\begin{equation}\n",
        "    J(\\Theta) = - \\sum_{i=1}^m y^{(i)} \\log \\left(h_\\Theta (x^{(i)})\\right) + (1-y^{(i)}) \\log \\left(1-h_\\Theta (x^{(i)})\\right),\n",
        "\\end{equation}\n",
        "\n",
        "assuming $0\\log(0)=0$.\n",
        "\n",
        "Use logistic regression to distinguish\n",
        "  _Versicolor_ and _Virginica_ irises. Use only the\n",
        "  `petal length` and `petal width` features. Use either\n",
        "  Gradient Descent, or L-BFGS to solve for the optimal $\\Theta$.\n",
        "  Prepare the scatterplot of the data and plot the class separation\n",
        "  boundary found by logistic regression."
      ]
    },
    {
      "metadata": {
        "id": "4i386u8IQe_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print 'Features: ', iris.feature_names\n",
        "print 'Targets: ', iris.target_names\n",
        "petal_length = iris.data[:,iris.feature_names.index('petal length (cm)')]\n",
        "petal_width = iris.data[:, iris.feature_names.index('petal width (cm)')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f84UE6eNQe_O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Extract the petal_length and petal_width of versicolors and virginicas\n",
        "\n",
        "IrisX = np.vstack([np.ones_like(petal_length), petal_length, petal_width])\n",
        "IrisX = IrisX[:, iris.target!=0]\n",
        "\n",
        "# Set versicolor=0 and virginica=1\n",
        "IrisY = (iris.target[iris.target!=0]-1).reshape(1,-1).astype(np.float64)\n",
        "\n",
        "scatter(IrisX[1,:], IrisX[2,:], c=IrisY.ravel(), cmap='spring')\n",
        "xlabel('petal_length')\n",
        "ylabel('petal_width')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "idZky448Qe_Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Our Gradient Descent implementation and lBFSG-B take as argumet a function, or more generally\n",
        "# a callable - i.e. something that you can call. Implement below a callable for logistic regression.\n",
        "#\n",
        "\n",
        "class LogisticRegressionCost(object):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "    \n",
        "    #note: this creates a Pyton callable - i.e. an object that can be called as a function\n",
        "    def __call__(self, Theta):\n",
        "        X = self.X\n",
        "        Y = self.Y\n",
        "        \n",
        "        #reshape Theta\n",
        "        ThetaR = Theta.reshape(X.shape[0],1)\n",
        "        \n",
        "        nll = TODO\n",
        "        \n",
        "        grad = TODO\n",
        "        \n",
        "        #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
        "        return nll, grad.reshape(Theta.shape)\n",
        "\n",
        "iris_log_reg_cost = LogisticRegressionCost(IrisX, IrisY)\n",
        "\n",
        "#TODO: add afew gradient checks as in the code above!\n",
        "\n",
        "Theta0 = np.zeros((3))\n",
        "\n",
        "#\n",
        "# Call a solver. You can use lBFSG-B or gradient descent. \n",
        "# Logistic regression cost is convex and lBFSG-B will work wery well on it.\n",
        "# However, neural network costs are not convex and we will use GD variants\n",
        "# more often with them.\n",
        "#\n",
        "\n",
        "ThetaOpt = TODO\n",
        "\n",
        "#\n",
        "# Now plot the found separation line \n",
        "# \n",
        "\n",
        "scatter(IrisX[1,:], IrisX[2,:], c=IrisY.ravel(), cmap='spring')\n",
        "xlabel('petal_length')\n",
        "ylabel('petal_width')\n",
        "pl_min, pl_max = xlim()\n",
        "pl = linspace(pl_min, pl_max, 1000)\n",
        "plot(pl, -(ThetaOpt[0]+ThetaOpt[1]*pl)/ThetaOpt[2])\n",
        "xlim(pl_min, pl_max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PAKaQGSmQe_i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Backpropagation through a *tanh* Neuron [2p]\n",
        "\n",
        "\n",
        "In the following assignments let:\n",
        " * $X \\in \\mathbb{R}^{k\\times N}$ be the data matrix containing $N$\n",
        "  samples each described with $k$ features. The $i$-th sample $x^{(i)} \\in\n",
        "  \\mathbb{R}^{(k\\times 1)}$ is the $i$-th column of $X$.\n",
        " * $Y \\in \\mathbb{R}^{1\\times N}$ be the row-vector of targets,\n",
        "  with $y^{(i)}$ being the target for the $i$-th sample.\n",
        " * $\\Theta\\in\\mathbb{R}^{k\\times 1}$ be the vector of parameters.\n",
        "\n",
        "\n",
        "  We want to use a single neuron with the $\\tanh(x) = \\frac{e^x -\n",
        "    e^{-x}}{e^x + e^{-x}}$ activation function.\n",
        "  First find the derivative $\\frac{\\partial \\tanh(x)}{\\partial\n",
        "    x}$ and express it as a function of $\\tanh(x)$.\n",
        "  Forward computations performed by the neuron are:\n",
        "  \n",
        "  \\begin{align*}\n",
        "    A &= \\Theta^T X \\\\\n",
        "    \\hat{Y} &= \\tanh(A) \\text{ applied elementwise} \\\\\n",
        "    E &= Y - \\hat{Y} \\\\\n",
        "    J &= E \\cdot E^T\n",
        "  \\end{align*}"
      ]
    },
    {
      "metadata": {
        "id": "MfriGqqJQe_k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**[2p]**  Find and express using matrix notation the following gradients. You\n",
        "  can refer to values and gradients computed earlier in the expressions for the\n",
        "  following ones -- just as you would when implementing a computer\n",
        "  program. Use $\\odot$ for the elementwise multiplication of matrices.\n",
        "\n",
        "  \\begin{align*}\n",
        "    \\frac{\\partial J}{\\partial E } &= ? \\\\\n",
        "    \\frac{\\partial J}{\\partial \\hat{Y}} &= ? \\\\\n",
        "    \\frac{\\partial J}{\\partial A} &= ? \\\\\n",
        "    \\frac{\\partial J}{\\partial \\Theta} &= ? \\\\\n",
        "  \\end{align*}\n",
        "  \n",
        "  **Note:** each gradient above should be implementable as a\n",
        "  compact expression in Python+NumPy.\n",
        "\n",
        "  **Hint:** write down the shapes of all values that you\n",
        "  compute. Work out the expressions for a single element of the\n",
        "  gradient, then see how they can be expressed using the matrix\n",
        "  notation."
      ]
    },
    {
      "metadata": {
        "id": "j702E-3aQe_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 4: SoftMax Regression [2p]\n",
        "\n",
        "The samples in the Iris dataset belong to one of three classes, while in\n",
        "CIFAR10 and MNIST they belong to one of 10 classes. Thus, linear regression cannot be\n",
        "applied because it distinguishes between two classes only.\n",
        "We will use SoftMax regression instead.\n",
        "\n",
        "Let $x\\in \\mathbb{R}^n$ be a sample vector and\n",
        "$y\\in \\{1,2,\\ldots,K\\}$ its class label.\n",
        "Similarly to what has been done during the lecture,\n",
        "we extend vector $x$ with the bias term $x_0=1$ to simplify the calculations\n",
        "(so now $x\\in \\mathbb{R}^{n+1}$).\n",
        "\n",
        "In SoftMax regression, we model conditional probability, that \n",
        "a given sample $x$ belongs to class $k$. Such model is parametrized\n",
        "with a matrix $\\Theta\\in\\mathbb{R}^{K \\times n+1}$.\n",
        "Note that in SoftMax regression, a separate linear model is build for each\n",
        "class. First we compute the vector $a$ of total inputs:\n",
        "\\begin{equation}\n",
        "a_k = \\sum_{j=0}^{n}\\Theta_{k,j}x_j,\n",
        "\\end{equation}\n",
        "or using matrix notation $a = \\Theta x$.\n",
        "Then we compute conditional probabilities using SoftMax regression:\n",
        "\\begin{equation}\n",
        "p(\\text{class}=k|x, \\Theta)= o_k = \\frac{\\exp{a_k}}{\\sum_{j=1}^K \\exp{a_j}}.\n",
        "\\end{equation}\n",
        "\n",
        "Function SoftMax transforms a $K$-element vector of real numbers\n",
        "to a vector of non-negative numbers which sum to 1. Thus they can be\n",
        "treated as probabilities assigned to $K$ separate classes.\n",
        "\n",
        "As it is the case with linear regression, we use cross-entropy\n",
        "as the loss function in SoftMax regression:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "J^{(i)}(\\Theta) &= - \\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)} \\\\\n",
        "J(\\Theta) &= \\frac{1}{m}\\sum_{i=1}^m J^{(i)}(\\Theta)= -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)} \n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "where $[y^{(i)}=k]$ equals $1$ when the $i-$th sample belongs to class $k$,\n",
        "and $0$ otherwise.\n",
        "Value $[y^{(i)}=k]$ might be interpreted as the correct value of the $k$-th\n",
        "output of the model on sample $i$.\n",
        "In addition, the total loss is expressed as a mean loss of particular samples,\n",
        "to make it independent of the size of the training set.\n",
        "\n",
        "Loss function gradient with respect to total inputs $a$ is simple:\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} = o_k^{(i)} - [y^{(i)}=k].\n",
        "\\end{equation}\n",
        "\n",
        "Using the chain rule, the gradient of the loss function with respect to\n",
        "model parameters becomes:\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial J}{\\partial \\Theta_{kj}} = \\sum_{i=1}^m \\frac{\\partial J}{\\partial J^{(i)}}\\frac{\\partial J^{(i)}}{\\partial \\Theta_{kj}} = \\sum_{i=1}^m \\frac{1}{m}\\cdot \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} \\frac{\\partial a^{(i)}_k}{\\partial \\Theta_{kj}} = \\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} x^{(i)}_j.\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "xINoRKfjQe_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "1. **[2p]**\n",
        "  Implement SoftMax regression and apply it to the Iris dataset.\n",
        "  During training, use L-BFGS from `scipy.optimize`. You can initialize the algorithm\n",
        "  with a null matrix (all entries being zeros).\n",
        "  Obtained accuracy should be comparable with that of k-NN\n",
        "  (roughly 3% of errors).\n",
        "  If your model doesn't work, check the gradient using the `check_gradient`\n",
        "  routine from the Starter Code of Assignment 3, which computes the gradient numerically."
      ]
    },
    {
      "metadata": {
        "id": "nlqnNjyLQe_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Here we load the IRIS dataset.\n",
        "# We will create two datasets:\n",
        "#  - one using all features,\n",
        "#  - one using just Petal Langth and Petal Width for visualizations.\n",
        "#\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "petal_length = iris.data[:, iris.feature_names.index('petal length (cm)')]\n",
        "petal_width = iris.data[:, iris.feature_names.index('petal width (cm)')]\n",
        "\n",
        "IrisXFull = np.vstack([np.ones_like(petal_length), iris.data.T])\n",
        "IrisX2feats = np.vstack(\n",
        "    [np.ones_like(petal_length), petal_length, petal_width])\n",
        "IrisY = iris.target.reshape(1, -1).astype(np.int64)\n",
        "\n",
        "print \"IrisXFull is a %s-shaped matrix of %s\" % (IrisXFull.shape, IrisXFull.dtype)\n",
        "print \"IrisX2feats is a %s-shaped matrix of %s\" % (IrisX2feats.shape, IrisX2feats.dtype)\n",
        "print \"IrisY is a %s-shaped matrix of %s\" % (IrisY.shape, IrisY.dtype)\n",
        "\n",
        "scatter(IrisX2feats[1, :], IrisX2feats[2, :], c=IrisY.ravel(), cmap='spring')\n",
        "xlabel('petal_length')\n",
        "ylabel('petal_width')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dtEpFBx1Qe_3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def SoftMaxRegression_implementation(ThetaFlat, X, Y=None, return_probabilities=False):\n",
        "    \"\"\"\n",
        "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ThetaFlat : \n",
        "        flat array of parameters containing (n_features*n_classes) entries\n",
        "    X :\n",
        "        array of features, shape n_features x n_smaples\n",
        "    Y :\n",
        "        optional array of desired targets of shape 1 x n_samples\n",
        "    return_probabilities : \n",
        "        if True, the probabilities are returned and Y is not used\n",
        "        if False, the los and gradient is computed on the X,Y pairs\n",
        "    \"\"\"\n",
        "    # X is num_features x num_samples\n",
        "    num_features, num_samples = X.shape\n",
        "\n",
        "    # Theta is num_features x num_classes\n",
        "    # we first reshape ThetaFlat into Theta\n",
        "    Theta = ThetaFlat.reshape(num_features, -1)\n",
        "\n",
        "    # Activation of softmax neurons\n",
        "    # A's shape should be num_classes x num_samples\n",
        "    #\n",
        "    # TODO\n",
        "    # A =\n",
        "    #\n",
        "\n",
        "    # Now compute the SoftMax function\n",
        "    # O will be a num_classes x num_samples matrix of probabilities assigned by our model\n",
        "    # Stability optimization - for each subtract the maximum activation\n",
        "    O = A - A.max(0, keepdims=True)\n",
        "    #\n",
        "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
        "    #        would sum to 1.0.\n",
        "    #\n",
        "\n",
        "    if return_probabilities:\n",
        "        return O\n",
        "\n",
        "    # The loss is the average per-sample nll (neg log likelihood)\n",
        "    # The nll is the sum of the logarithms of probabilities assigned to each class\n",
        "    correct_class_likelihoods = np.log(O[Y.ravel(), np.arange(num_samples)])\n",
        "    L = - 1.0 / num_samples * np.sum(correct_class_likelihoods)\n",
        "\n",
        "    # For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
        "    # Please fill in its computation\n",
        "    # Don't forget to normalize the gradient by num_samples.\n",
        "    #\n",
        "    # TODO\n",
        "    # dLdA =\n",
        "    #\n",
        "\n",
        "    # Now we compute the gradient of the loss with respect to Theta\n",
        "    dLdTheta = np.dot(X, dLdA.T)\n",
        "\n",
        "    # reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
        "    return L, dLdTheta.reshape(ThetaFlat.shape)\n",
        "\n",
        "\n",
        "# Make a function for training on irises\n",
        "def iris_log_reg_cost(Theta): return SoftMaxRegression_implementation(\n",
        "    Theta, IrisXFull, IrisY, False)\n",
        "\n",
        "\n",
        "# Make sure that the gradient computation is OK\n",
        "check_gradient(iris_log_reg_cost, np.zeros((3 * 5,)))\n",
        "check_gradient(iris_log_reg_cost, np.random.rand(3 * 5) * 2.0 - 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_MAQ8Z3UQfAE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Call the solver\n",
        "#\n",
        "\n",
        "# NOTE: iprint will cause the solver to print TO THE TERMINAL\n",
        "#       from which ipython notebook was started\n",
        "ThetaOpt = sopt.fmin_l_bfgs_b(\n",
        "    iris_log_reg_cost, np.zeros((3 * 5,)), iprint=1)[0]\n",
        "\n",
        "check_gradient(iris_log_reg_cost, ThetaOpt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zEt47ePgQfAQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Compute training errors\n",
        "#\n",
        "\n",
        "probabilities = SoftMaxRegression_implementation(\n",
        "    ThetaOpt, IrisXFull, return_probabilities=True)\n",
        "predictions = np.argmax(probabilities, 0)\n",
        "\n",
        "print \"Training accurracy: %f%%\" % ((predictions == IrisY.ravel()).mean() * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Pan7dmIQfAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Now redo the training for two features\n",
        "#\n",
        "# TODO: again, use l_bfgs to find optimal theta, \n",
        "#       then compute probabilities and new predictions.\n",
        "#\n",
        "\n",
        "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7HoJgkNQfAg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Now plot the decision boundary\n",
        "#\n",
        "\n",
        "petal_lengths, petal_widths = np.meshgrid(\n",
        "    np.linspace(IrisX2feats[1, :].min(), IrisX2feats[1, :].max(), 100),\n",
        "    np.linspace(IrisX2feats[2, :].min(), IrisX2feats[2, :].max(), 100))\n",
        "\n",
        "IrisXGrid = np.vstack([np.ones(np.prod(petal_lengths.shape)),\n",
        "                       petal_lengths.ravel(), petal_widths.ravel()])\n",
        "predictions_Grid = SoftMaxRegression_implementation(\n",
        "    Theta2class, IrisXGrid, return_probabilities=True).argmax(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8M_Az-3QfAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contourf(petal_lengths, petal_widths, predictions_Grid.reshape(petal_lengths.shape), cmap='spring')\n",
        "scatter(IrisX2feats[1,:], IrisX2feats[2,:], c=IrisY.ravel(), cmap='spring')\n",
        "xlabel('petal_length')\n",
        "ylabel('petal_width')\n",
        "title('Decision boundary found by SoftMAx regression')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKwu_DYNQfAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 5: 2-layer Neural Network [2p]\n",
        "\n",
        "The task is to extend the SoftMax regression model to a 2-layer neural net.\n",
        "The network will transform an input vector to an activation vector\n",
        "of hidden neurons and finally, using the SoftMax function,\n",
        "to a vector of probabilities of the sample's belonging to one of 10 classes.\n",
        "\n",
        "To train the network, we'll need the loss function $J$ and its gradient\n",
        "with respect to network's parameters (weights and biases).\n",
        "For a 2-layer net, this can be achieved using the following relationships:\n",
        "\n",
        "### Data\n",
        "\n",
        "The training set has $m$ samples of $n$ dimensions, belonging to one\n",
        "of $K$ classes, it is given as a set of matrices: $X \\in \\mathbb{R}^{n\\times m}$\n",
        "and $Y\\in \\{1,2,\\ldots,K\\}^{1\\times m}$.\n",
        "\n",
        "### Parameters\n",
        "\n",
        "The net will have 2 layers: 1) a hidden one, having $L$ neurons,\n",
        "and 2) an output one, having $K$ neurons (one for each of $K$ classes).\n",
        "The layers are defined through:\n",
        "\n",
        "\n",
        "1. the parameters of the hidden layer, which maps $n$-dimensional input vectors\n",
        "  into activations of $L$ neurons:\n",
        "  weight matrix $W^h\\in\\mathbb{R}^{L\\times n}$ and bias vector\n",
        "  $b^h\\in\\mathbb{R}^{L\\times 1}$,\n",
        "  \n",
        "2. the parameters of the output layer, which maps $L$-dimensional vector\n",
        "  of activations of the hidden layer to $K$ activations of output neurons:\n",
        "  weight matrix $W^o\\in{K\\times L}$ and bias vector $b^o\\in\\mathbb{R}^{K\\times 1}$.\n",
        "\n",
        "### Signal forward propagation (fprop)\n",
        "\n",
        "Each hidden neuron computes its total input as a sum of product of its\n",
        "inputs, weight matrix and bias. For an $i$-th sample,\n",
        "the total input\n",
        "${a^{h}}^{(i)}_l $ of an $I$-th neuron is thus:\n",
        "\\begin{equation}\n",
        "{a^h}^{(i)}_l = \\sum_{j=1}^n {W^h}_{l,j}x^{(i)}_j + {b^h}_l\n",
        "\\end{equation}\n",
        "The total input of neurons might also be expressed via matrices,\n",
        "using matrix multiplication and broadcasting (which allows to add\n",
        "a column vector to all column vectors of a matrix):\n",
        "\\begin{equation}\n",
        "{a^h} = W^h\\cdot x + b^h\n",
        "\\end{equation}\n",
        "This can be implemented in Python as `ah = W.dot(x) + b`.\n",
        "\n",
        "Next, we compute activation $h^h$ of hidden neurons with hyperbolic tangent\n",
        "$\\tanh(a) = \\frac{e^a-e^{-a}}{e^a+e^{-a}}$:\n",
        "\\begin{equation}\n",
        "{h^h}^{(i)}_l=\\tanh({a^h}^{(i)}_l)\n",
        "\\end{equation}\n",
        "Thanks to vectorization in Python + numpy, $h^h$ might be computed with a single\n",
        "expression `hh = numpy.tanh(ah)`.\n",
        "\n",
        "Total input of the output layer can be computed using\n",
        "activations of the hidden layer (with the help of broadcasting) as:\n",
        "\n",
        "\\begin{equation}\n",
        "a^o = W^o\\cdot h^h + b^o\n",
        "\\end{equation}\n",
        "\n",
        "Finally, probabilities of a sample's belonging to  particular classes\n",
        "have to be computed. This can be achieved with SoftMax:\n",
        "\n",
        "\\begin{equation}\n",
        "    p(y^{(i)}=k|x^{(i)}) = o^{(i)}_k = \\frac{\\exp({a^o}^{(i)}_k)}{ \\sum_{k'=1}^K \\exp( {a^o}^{(i)}_{k'} )}.\n",
        "\\end{equation}\n",
        "\n",
        "Like with SoftMax regression, we will use cross-entropy\n",
        "as the loss function:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "J^{(i)}(\\Theta) &= - \\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)}, \\\\\n",
        "J(\\Theta) &= \\frac{1}{m}\\sum_{i=1}^m J^{(i)}(\\Theta)= -\\frac{1}{m}\\sum_{i=1}^n\\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)}.\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "### Error backpropagation (bprop)\n",
        "\n",
        "Using the chain rule one can derive the gradient of the loss function\n",
        "in respect to neurons' activations and network parameters.\n",
        "\n",
        "\n",
        "First we compute the gradient with respect to the output layer's\n",
        "total inputs:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} = \\frac{1}{m}(o_k^{(i)} - [y^{(i)}=k]),\n",
        "\\end{equation}\n",
        "\n",
        "then we compute the gradient with respect to activations of hidden units:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial J}{\\partial {h^h}^{(i)}_l} = \\sum_{k=1}^K \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} \\frac{\\partial {a^o}^{(i)}_k}{\\partial {h^h}^{(i)}_l} =  \\sum_{k=1}^K \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} {W^o}_{kl},\n",
        "\\end{equation}\n",
        "then we compute the gradient with respect to the total activations of hidden units:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial J}{\\partial {a^h}^{(i)}_l} = \\frac{\\partial J}{\\partial {h^h}^{(i)}_l}\\frac{\\partial {h^h}^{(i)}_l}{\\partial {a^h}^{(i)}_l} = \\frac{\\partial J}{\\partial {h^h}^{(i)}_l} (1 - ({h^h}^{(i)}_l)^2),\n",
        "\\end{equation}\n",
        "\n",
        "where we have used the relationship\n",
        "\n",
        "$\\frac{\\partial \\tanh(x)}{\\partial x} = 1-\\tanh(x)^2$.\n",
        "\n",
        "Finally we can use the gradients with respect to the total inputs to\n",
        "compute the gradients with respect to network parameters,\n",
        "eg. for the input layer:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial J}{\\partial {W^o}_{kl}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}\\frac{\\partial {a^o}^{(i)}_k}{\\partial {W^o}_{kl}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}{h^h}^{(i)}_l,\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial J}{\\partial {b^o}_{k}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}\\frac{\\partial {a^o}^{(i)}_k}{\\partial {b^o}_{k}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}.\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "TrJXZkhXQfAu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "1. **[1.5p]**\n",
        "  Implement a 2-layer neural network as a function\n",
        "  **TwoLayerNet($\\Theta$,X,Y)**\n",
        "  which computes the loss and gradient of loss with\n",
        "  respect to the weights and bias terms (encoded as $\\Theta$)\n",
        "  on data given as $X$ and $Y$.\n",
        "  Refer to the Starter Code below for the details.\n",
        "  Try to express as much as possible with matrix calculus.\n",
        "\n",
        "2. **[0.5p]**\n",
        "  In the cases of linear and logistic regression,\n",
        "  we could start the optimization with a vector of zeros.\n",
        "  Such initialization will be troublesome for neural networks.\n",
        "\n",
        "  You can use the following initialization methods for\n",
        "  network parameters: a) initialize weight matrices with small random\n",
        "  numbers (eg. drawn from $\\mathcal{N}(0, 0.2)$), b) initialize bias\n",
        "  vectors with zeros. Train the network on the Iris dataset and report\n",
        "  classification accuracy.\n",
        "\n",
        "\n",
        "  The following problems will require to train the network.\n",
        "  Use the L-BFGS optimizer from `scipy.optimize` to minimize\n",
        "  your function (particularly: the loss) and find the right $\\Theta$."
      ]
    },
    {
      "metadata": {
        "id": "PAW6v9TjQfAw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with httpimport.github_repo('janchorowski', 'nn_assignments', \n",
        "                            module='common', branch='nn18'):\n",
        "    from common.gradients import check_gradient, encode_params, decode_params\n",
        "\n",
        "\n",
        "def TwoLayerNet_implementation(ThetaFlat, ThetaShapes, X, Y=None, return_probabilities=False):\n",
        "    \"\"\"\n",
        "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    ThetaFlat : \n",
        "        flat array of parameters\n",
        "    ThetaShapes :\n",
        "        list of shapes of weight and bias matrices\n",
        "    X :\n",
        "        array of features, shape n_features x n_smaples\n",
        "    Y :\n",
        "        optional array of desired targets of shape 1 x n_samples\n",
        "    return_probabilities : \n",
        "        if True, the probabilities are returned and Y is not used\n",
        "        if False, the los and gradient is computed on the X,Y pairs\n",
        "    \"\"\"\n",
        "    #X is num_features x num_samples\n",
        "    num_features, num_samples = X.shape\n",
        "\n",
        "    #Extract weight matrices\n",
        "    W1, W2 = decode_params(ThetaFlat, ThetaShapes)\n",
        "    \n",
        "    X_padded = np.vstack([np.ones((1, num_samples)), X])\n",
        "    \n",
        "    #Activation in first layer. Shape is num_hidden x num_samples\n",
        "    #\n",
        "    # TODO\n",
        "    # A1 = \n",
        "    #\n",
        "\n",
        "    #Apply the transfer function\n",
        "    #\n",
        "    # TODO\n",
        "    # H1 = \n",
        "    #\n",
        "        \n",
        "    #Pad with ones\n",
        "    H1_padded = np.vstack([np.ones((1, num_samples)), H1])\n",
        "    \n",
        "    #Now apply the second linear transform\n",
        "    #\n",
        "    # TODO\n",
        "    # A2 = \n",
        "    #\n",
        "    \n",
        "    #Now compute the SoftMax function\n",
        "    #O will be a num_classes x num_samples matrix of probabilities assigned by our model  \n",
        "    #Stability optimization - for each subtract the maximum activation\n",
        "    O = A2 - A2.max(0, keepdims=True)\n",
        "    # \n",
        "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
        "    #        would sum to 1.0.\n",
        "    # \n",
        "\n",
        "    if return_probabilities:\n",
        "        return O\n",
        "    \n",
        "    #The loss is the average per-sample nll (neg log likelihood)\n",
        "    #The nll is the sum of the logarithms of probabilities assigned to each class\n",
        "    correct_class_likelihoods = np.log(O[Y.ravel(), np.arange(num_samples)])\n",
        "    L = - 1.0/num_samples * np.sum(correct_class_likelihoods)\n",
        "\n",
        "    #For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
        "    #Please fill in its computation\n",
        "    #\n",
        "    # TODO\n",
        "    # dLdA2 = \n",
        "    #\n",
        "\n",
        "    dLdH1_padded = W2.dot(dLdA2)\n",
        "    dLdH1 = dLdH1_padded[1:,:] #ship the derivatives backpropagated to the added ones\n",
        "    \n",
        "    #\n",
        "    # TODO - compute the derivatives dLdW2 and dLdW1\n",
        "    # Hint - to compute dLdW1, start with dLdA1\n",
        "    #\n",
        "    \n",
        "    dLdThetaFlat, unused_shapes = encode_params([dLdW1, dLdW2])\n",
        "    \n",
        "    #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
        "    return L, dLdThetaFlat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sScSZnryQfAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# Here we initialize the network for gradient testing on IRIS\n",
        "#\n",
        "# We will have 7 hidden neurons.\n",
        "# The first weight matrix will be 5 (4 features + bias) x 7 (hidden neurons)\n",
        "# The second weight matrix will be 8 (7 neurons + bias) x 3 (classes)\n",
        "# you can initialize the weight matrices from a Gaussian distribution with mean 0 and stdev 0.2.\n",
        "#\n",
        "num_hidden = 7\n",
        "#\n",
        "# TODO\n",
        "# W1 = \n",
        "# W2 = \n",
        "#\n",
        "\n",
        "# Now flatten into an array\n",
        "Theta0, ThetaShape = encode_params([W1,W2])\n",
        "\n",
        "#Make a function for training on irises\n",
        "iris_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, iris.data.T, IrisY, False)\n",
        "#Make sure that the gradient computation is OK\n",
        "check_gradient(iris_net_cost, Theta0)\n",
        "check_gradient(iris_net_cost, np.zeros_like(Theta0))\n",
        "check_gradient(iris_net_cost, np.ones_like(Theta0)*0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_NNcEg2QQfA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#\n",
        "# TODO - apply L-BFGS to minimize the loss and get optimal ThetaOpt.\n",
        "#\n",
        "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, iris.data.T, return_probabilities=True).argmax(0)\n",
        "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "injCJeIzQfA-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 6: XOR, Iris and MNIST [2p + 2b]\n",
        "\n",
        "1. **[2p]** Solve the following with a 2-layer neural network:\n",
        "    1. Test your network on a 2-dimensional and a 3-dimensional\n",
        "    XOR problem. How many hidden neurons the network requires to express\n",
        "    the XOR function?\n",
        "    2. Normalize the Iris dataset, so that each of the 4 attributes\n",
        "    would fall into [-1,1] interval. Train the network and check classification\n",
        "    accuracy. \n",
        "2. **[1p bonus]** Overfit to the data by reaching 100% *training*\n",
        "    accuracy on the MNIST dataset. Remember to normalize the data.\n",
        "3. **[1p bonus]** Plot samples (for XOR or for Iris) in hidden neurons' activation space\n",
        "    (similarly to http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)."
      ]
    },
    {
      "metadata": {
        "id": "Xxc5bleNQfBA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "XOR2X = np.array([[0,0],\n",
        "                  [0,1],\n",
        "                  [1,0],\n",
        "                  [1,1]]).T\n",
        "XOR2Y = np.array([[0,1,1,0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WeR2RkmYQfBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#init the neurons\n",
        "num_hidden = 2\n",
        "W1 = (np.random.rand(3,num_hidden) - 0.5)\n",
        "W2 = (np.random.rand(num_hidden+1,2) -0.5)\n",
        "\n",
        "# Now flatten into an array\n",
        "Theta0, ThetaShape = encode_params([W1,W2])\n",
        "\n",
        "#\n",
        "# TODO - apply L-BFGS to minimize the loss and get optimal ThetaOpt.\n",
        "#\n",
        "\n",
        "TwoLayerNet_implementation(ThetaOpt, ThetaShape, XOR2X, return_probabilities=True).argmax(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BMAHcce8QfBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# TODO - repeat the experiment for 3-dimensional XOR.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gq9QaGQYQfBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IrisNormX = np.array(iris.data.T)\n",
        "#\n",
        "# TODO - normalize IrisNormX, so the vlaues would fall into [-1,1].\n",
        "#        Avoid looping constructs.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfQi9kUkQfBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# TODO - set the parameters\n",
        "#\n",
        "# num_hidden = \n",
        "# W1 = \n",
        "# W2 = \n",
        "\n",
        "Theta0, ThetaShape = encode_params([W1, W2])\n",
        "\n",
        "#\n",
        "# TODO - train the network\n",
        "#\n",
        "\n",
        "predictions = TwoLayerNet_implementation(\n",
        "    ThetaOpt, ThetaShape, IrisNormX, return_probabilities=True).argmax(0)\n",
        "print \"Training accurracy: %f%%\" % ((predictions == IrisY.ravel()).mean() * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbPsIz4VQfBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with np.load('mnist.npz') as data:\n",
        "    mnist_full_train_data_uint8 = data['train_data']\n",
        "    mnist_full_train_labels_int64 = data['train_labels']\n",
        "    mnist_test_data_uint8 = data['test_data']\n",
        "    mnist_test_labels_int64 = data['test_labels']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LAz89cSNQfBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# TODO - normalize the data\n",
        "#\n",
        "\n",
        "#\n",
        "# TODO - set parameters, train on the MNIST dataset\n",
        "#        and report training accuracy\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3EZ-YXryQfBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# (Bonus)\n",
        "# TODO - change network implementation code to return hidden activations.\n",
        "# Hint - locals() gives the dictionary of all objects in a functions's scope!\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLKlDuO_QfB3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 7 [1p]\n",
        "\n",
        "Answer the following:\n",
        " \n",
        "   * What will happen if for each layer (hidden and output) all weights\n",
        "    will be initialized to the same values before the training?\n",
        "    \n",
        "   * How will the value of SoftMax function change,\n",
        "  if we will add the same constant term to each element of $a$?\n",
        "  Often, before computing SoftMax, the largest value can be subtracted\n",
        "  to mitigate large exponents and associated numerical errors.\n",
        "  Is it a good practice?\n",
        "\n",
        "   * Are two-class SoftMax regression and logistic regression equivalent (can you build a logistic regression model from a given softmax one and vice versa)?"
      ]
    },
    {
      "metadata": {
        "id": "iS31vZRzQfB5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 8 [3p bonus]\n",
        "\n",
        "  The least squares method results in estimates that approximate\n",
        "  the conditional mean of the response variable given certain values\n",
        "  of the predictor variables.  However, for many applications we are interested\n",
        "  in a median or other percentile rather than the mean of the response\n",
        "  variable. An approximation of a percentile can be achieved\n",
        "  indirectly by using the least squares method to fit a model,\n",
        "  assuming a gaussian distribution on residuals of this model and\n",
        "  calculating the required percentile of the gaussian distribution.\n",
        "  The problems with this commonly used technique appear when the\n",
        "  distribution of residuals does not follow a gaussian distribution.\n",
        "\n",
        "  The quantile regression\n",
        "  (https://en.wikipedia.org/wiki/Quantile_regression) aims at\n",
        "  directly estimating a value of the conditional percentile of the\n",
        "  response variable. It is often use in e.g. sales forecasting, where\n",
        "  we are interested in e.g. keeping enough items to have a 90% chance\n",
        "  to cover the demand. In quantile regression, instead of minimizing the mean squared error,\n",
        "  quantile regression minimizes a different cost function,\n",
        "  which for pair $(x_i,y_i)$ is\n",
        "  \\begin{equation}\n",
        "  f^{\\tau}_i = \\left\\{\n",
        "    \\begin{array}{rl}\n",
        "      \\tau\\cdot e_i & \\text{if } e_i \\geq 0,\\\\\n",
        "      -(1-\\tau)\\cdot e_i & \\text{if } e_i < 0,\n",
        "    \\end{array} \\right.\n",
        "  \\end{equation}\n",
        "  where $\\tau$ is the precentile of interest, and $e_i=\\theta^Tx_i - y_i$ is the residual.\n",
        "\n",
        "1. **[2p bonus]** Download the house pricing data set from\n",
        "    http://ii.uni.wroc.pl/~jch/neuralnetworks15/lecture3/03-house-prices.csv. To\n",
        "    load it into Python, you can use the `pandas.read_csv`\n",
        "    function, similarly to how we read the height/weight table in the\n",
        "    notebook for lecture 3.\n",
        "\n",
        "    This data contains information about areas and prices of around\n",
        "    5000 houses offered for sale. Imagine you want to buy a 60 squared\n",
        "    meters flat. How much money do you need to have to be able to\n",
        "    choose between $75\\%$ of all flats offered for sale?\n",
        "    \n",
        "    Solve this problem using both methods described above and compare\n",
        "    the results. \n",
        "    \n",
        "    Fit a linear regression model to the dependance of a house price\n",
        "    on its area using the least squares method. Fit a Gaussian\n",
        "    distribution to the residuals and calculate the 75 percentile of\n",
        "    this distribution (you can look it up in a percentile table, or\n",
        "    use the `scipy.stats.norm.ppf` function). Check the goodness of fit\n",
        "    of your model by calculating the ratio of prices below the\n",
        "    estimated 75 percentile. Understand the reason of the quality of\n",
        "    the model by plotting a histogram of residuals and noting that\n",
        "    they do not seem to follow a Gaussian distribution.\n",
        "  \n",
        "    Fit a quantile regression model by minimizing the $f^{\\tau}$\n",
        "    function. To fit the model write a function returning the cost,\n",
        "    and its derivative (you can assume that the derivative is 0 at the\n",
        "    singular points).  Use the L-BFGS solver (without step size\n",
        "    control the GD algorithm does not converge to good solutions). You\n",
        "    can start the solver from the least squares solution. Check the\n",
        "    goodness of fit as described above.\n",
        "\n",
        "    Now you can calculate the 75 percentile price value for a 60\n",
        "    squared meters flat using the more adequate model. \n",
        "    \n",
        "    **Note**: Technically, we can not use a gradient-based\n",
        "    optimization method to minimize the $f^{\\tau}=\\sum_i f^{\\tau}_i$\n",
        "    loss because it\n",
        "    doesn't have a derivative for $e_i=0$. One solution is to use a\n",
        "    subgradient method\n",
        "    https://en.wikipedia.org/wiki/Subderivative. It is also\n",
        "    possible to use a smooth loss function -- common variants are the\n",
        "    Huber loss (https://en.wikipedia.org/wiki/Huber_loss) or a\n",
        "    variant of the smooth approximation to the absolute value\n",
        "    $\\text{abs}(x) \\approx \\sqrt{x^2+\\epsilon}$ with a small\n",
        "    $\\epsilon$. However, just ignoring the singular points tends to\n",
        "    work well in practice.\n",
        "  \n",
        "2. **[1p bonus]** Prove that the squared error minimization leads to\n",
        "    the approximation of the mean while the minimization of the $f^{\\tau}$\n",
        "    function leads to the estimation of the $\\tau$ percentile. \n",
        "\n",
        "    For a set of points on a plane find a horizontal line which minimizes the sum of\n",
        "    1. squares,\n",
        "    2. absolute values,\n",
        "    3. $f^{\\tau}$\n",
        "    \n",
        "    of vertical coordinates of the points.\n",
        "\n",
        "  **Hint**: you should remember from the Algorithms and Data\n",
        "  Structures course that the median minimizes a sum of absolute values\n",
        "  loss function."
      ]
    },
    {
      "metadata": {
        "id": "Wkf0xKXVGPeU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.optimize as sopt\n",
        "from scipy.stats import norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpnCtPzMGTdp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/janchorowski/nn_assignments/nn18/assignment3/03-house-prices-outliers.csv',\n",
        "    index_col=0)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vzB25x1GXE0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.vstack((np.ones_like(data.rooms), data.area))\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCp4rQYpGXu2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y = np.asarray(data.price)[None,:]\n",
        "Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRKf9O68GZTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Find the least squares (linear regression) solution\n",
        "ThetaLsq = TODO\n",
        "print ThetaLsq\n",
        "\n",
        "errs_lsq = ThetaLsq.T.dot(X) - Y\n",
        "\n",
        "# Assuming a normal residual distribution, compute the 25 and 75 percentile\n",
        "low_qtr, high_qtr = norm.ppf(TODO)\n",
        "\n",
        "# Verify the assumption - observe how many residuals are blow the 25th or 75th percentile\n",
        "print mean(errs_lsq), median(errs_lsq), std(errs_lsq)\n",
        "print \"Fraction of errs below 25 perc: %f, above 75 perc: %f\" % (\n",
        "    (errs_lsq<low_qtr).mean(), (errs_lsq>high_qtr).mean())\n",
        "\n",
        "scatter(data.area, data.price, alpha=0.2)\n",
        "xlim_area = xlim()\n",
        "XL = np.vstack(([1.0, 1.0], xlim_area))\n",
        "plot(xlim_area, XL.T.dot(ThetaLsq), label='LSQ')\n",
        "\n",
        "# Plot confidence intervals around the prediction: prediction + high_qtr, prediction - low_qtr \n",
        "plot(xlim_area, TODO, label='LSQ25')\n",
        "plot(xlim_area, TODO, label='LSQ75')\n",
        "legend()\n",
        "# ylim(-5000, 5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-YCJjlyGanS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def quant_loss(X, Y, Theta, left, right):\n",
        "    \"\"\"Computes the quantile loss and its derivative.\n",
        "    \n",
        "    The quantile loss is defined as:\n",
        "    \n",
        "    \\sum_i loss_i(err_i)\n",
        "    \n",
        "    where loss_i(err_i) = abs(err_i) * left if err_i < 0 abs(err_i) * right if err_i >= 0\n",
        "    \n",
        "    Args:\n",
        "        X: data matrix of shape d x N\n",
        "        Y: targets, shape 1 x N\n",
        "        Theta: parameters vector of shape N\n",
        "        left, right: coefficients for negative and positive errors\n",
        "        \n",
        "    Returns:\n",
        "        tuple of loss, gradient\n",
        "    \"\"\"\n",
        "    ThetaR = np.asarray(Theta).reshape((X.shape[0],1))\n",
        "    O = ThetaR.T.dot(X)\n",
        "    E = O - Y\n",
        "    loss = TODO\n",
        "    grad = TODO\n",
        "    return loss, grad.ravel()\n",
        "\n",
        "check_gradient(lambda Theta: quant_loss(X,Y,Theta,1,1), [0.,0.])\n",
        "check_gradient(lambda Theta: quant_loss(X,Y,Theta,1,1), ThetaLsq.ravel(), delta=1e-8)\n",
        "check_gradient(lambda Theta: quant_loss(X,Y,Theta,1,10), [0.,0.])\n",
        "check_gradient(lambda Theta: quant_loss(X,Y,Theta,1,1), [1.,1.])\n",
        "check_gradient(lambda Theta: quant_loss(X,Y,Theta,1,10), [1.,1.])\n",
        "\n",
        "scatter(data.area, data.price, alpha=0.2)\n",
        "xlim_area = xlim()\n",
        "XL = np.vstack(([1.0,1.0], xlim_area))\n",
        "plot(xlim_area, XL.T.dot(ThetaLsq), label='LSQ')\n",
        "\n",
        "for quant in [25, 50, 75]:\n",
        "    # Fill in proper values for left and right coefficients\n",
        "    ThetaQuant = sopt.fmin_l_bfgs_b(lambda Theta: quant_loss(X,Y,Theta, TODO), \n",
        "                                    [0,0])[0]\n",
        "    print \"Quant %d:\" % (quant)\n",
        "    print ThetaQuant\n",
        "    errs_quant = Y - ThetaQuant.T.dot(X)\n",
        "    print \"Fraction of negative errs: %f (should be %f)\" % ((errs_quant<0).mean(), quant/100.)\n",
        "    \n",
        "    plot(xlim_area, XL.T.dot(ThetaQuant), label='Q%d' %(quant,))\n",
        "\n",
        "xlim(0, 250)  #xlim(0, xlim_area[1])\n",
        "legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0zOcCIZjGdjJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
