{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Fuel](https://github.com/mila-udem/fuel) library to access datasets. Fuel is designed to provide easy iteration over large datasets, however in this assignment we will only use its ability to download and convert some standard datasets for us.\n",
    "\n",
    "We will use the following datasets:\n",
    "1. IRIS https://archive.ics.uci.edu/ml/datasets/Iris available as `fuel.datasets.iris.Iris`\n",
    "2. MNIST http://yann.lecun.com/exdb/mnist/ available as `fuel.datasets.mnist.MNIST`\n",
    "3. CIFAR10 http://www.cs.toronto.edu/~kriz/cifar.html available as `fuel.datasets.cifar10.CIFAR10`\n",
    "\n",
    "On lab computers the datasets have already been downloaded for you into the `/pio/data/data/fuel` directory. Make sure to add it to the `FUEL_DATA_PATH` environment variable!\n",
    "\n",
    "If you are working from your computer, you can either use the Fuel downloader and converter utulities (http://fuel.readthedocs.org/en/latest/built_in_datasets.html) or download the HDF5 datasets:\n",
    "\n",
    "1. [Iris](https://drive.google.com/uc?export=download&id=0B5j9vIO_Njwcb2ItV2ZLakR6MEk)\n",
    "2. [MNIST](https://drive.google.com/uc?export=download&id=0B5j9vIO_NjwcNnYzVTNIVGxaSEk)\n",
    "3. [CIFAR10](https://drive.google.com/uc?export=download&id=0B5j9vIO_NjwcOEdlU2RtNkc2bW8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading MNIST data\n",
    "\n",
    "from fuel.datasets.mnist import MNIST\n",
    "from common.plotting import plot_mat\n",
    "\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "\n",
    "print \"We have %d training, %d validation, and %d test examples\" % (\n",
    "    mnist_train.num_examples, mnist_validation.num_examples, mnist_test.num_examples)\n",
    "print \"The examples are pairs of %s:\" % (mnist_train.sources,)\n",
    "for i, source in enumerate(mnist_train.sources):\n",
    "    labels = mnist_train.axis_labels[source]\n",
    "    print 'The source #%d named \"%s\" is a %dd array with axis: %s' % (\n",
    "        i, source, len(labels), labels)\n",
    "\n",
    "#Note that for larger datasets that are loaded into mameory the data_sources field may not exist!\n",
    "plot_mat(mnist_train.data_sources[0][:20], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: plot an example of each class on MNIST and on CIFAR\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract the data matrices\n",
    "\n",
    "mnist_train_X = (mnist_train.data_sources[0].reshape(mnist_train.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_train_Y = mnist_train.data_sources[1].ravel()\n",
    "\n",
    "mnist_valid_X = (mnist_validation.data_sources[0].reshape(mnist_validation.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_valid_Y = mnist_validation.data_sources[1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def KNN(train_X, train_Y, test_X, test_Y, ks, batch_size=200):\n",
    "    \"\"\"\n",
    "    Compute error rate for various \n",
    "    \"\"\"\n",
    "    errs = np.zeros((len(ks),))\n",
    "    for i in xrange(0,test_Y.shape[0], batch_size):\n",
    "        batch_X = test_X[i:i + batch_size]\n",
    "        batch_Y = test_Y[i:i + batch_size]\n",
    "        print \"Examples %d:%d Computing distances... \" %(i,i+batch_size), \n",
    "        \n",
    "        #\n",
    "        # TODO: fill in an efficient distance matrix computation \n",
    "        #\n",
    "        dists = TODO\n",
    "        \n",
    "        print \"Sorting... \",\n",
    "        closest = np.argsort(dists,0)\n",
    "\n",
    "        print \"Computing errors...\"\n",
    "        targets = train_Y[closest]\n",
    "        \n",
    "        for ki,k in enumerate(ks):\n",
    "            predictions, unused_counts = mode(targets[:k,:], axis=0)\n",
    "            predictions = predictions.ravel()\n",
    "            #\n",
    "            # TODO: fill in error count computation\n",
    "            #\n",
    "            errs[ki] += TODO\n",
    "        \n",
    "    errs /= test_Y.shape    \n",
    "    return np.vstack((ks, errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now find the best k on the validation set\n",
    "\n",
    "mnist_validation_errs = KNN(mnist_train_X, mnist_train_Y, mnist_valid_X, mnist_valid_Y, [1,3,5,7,9])\n",
    "plot(mnist_validation_errs[0,:], mnist_validation_errs[1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now use the best k to compute the test error\n",
    "\n",
    "best_K =  TODO\n",
    "\n",
    "mnist_full_train = MNIST((\"train\",), )\n",
    "\n",
    "mnist_full_train_X = (mnist_full_train.data_sources[0].reshape(mnist_full_train.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_full_train_Y = mnist_full_train.data_sources[1].ravel()\n",
    "\n",
    "mnist_test_X = (mnist_test.data_sources[0].reshape(mnist_test.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_test_Y = mnist_test.data_sources[1].ravel()\n",
    "\n",
    "mnist_test_errs = KNN(mnist_full_train_X, mnist_full_train_Y, mnist_test_X, mnist_test_Y, [best_K])\n",
    "print \"When k=%d the test error rate is %.1f%%\" % (mnist_test_errs[0,0], mnist_test_errs[0,1]*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now repeat the k-NN training for CIFAR10\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Solve problem 2 here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common.gradients import check_gradient,numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A sample function for debugging purposes\n",
    "\n",
    "def quadratic(X):\n",
    "    value = X[0]**2 + 2*X[1]**2\n",
    "    grad = np.array([2*X[0], 2*X[1]]) # TODO\n",
    "    return (value, grad)\n",
    "\n",
    "\n",
    "# Let's check if the gradientis correctly computed\n",
    "check_gradient(quadratic, np.array([0,0]))\n",
    "check_gradient(quadratic, np.array([1.0,1.0]))\n",
    "\n",
    "# Oh noes! We have an error in gradient computation - please correct it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Fill in the details of the gradient computation\n",
    "#\n",
    "\n",
    "def GD(f, Theta0, alpha, stop_tolerance=1e-10, max_steps=1000000):\n",
    "    history = [Theta0]\n",
    "    \n",
    "    Theta = Theta0\n",
    "    value = np.inf\n",
    "    \n",
    "    step = 0\n",
    "    while step<max_steps:\n",
    "        previous_value = value\n",
    "        value, gradient = f(Theta)\n",
    "        if TODO: #fill in a stopping condition here\n",
    "            break\n",
    "        \n",
    "        Theta = TODO #fill in the gradient descent rule\n",
    "        history.append(Theta)\n",
    "        step += 1\n",
    "    return Theta, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Test the GD procedure on the quadratic function\n",
    "#\n",
    "\n",
    "Xopt, Xhist = GD(quadratic, np.array((1,1)), 1e-1)\n",
    "print \"Found optimum at %s in %d steps (true minimum is at [0,0])\" % (Xopt, len(Xhist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now implement the Rosenbrock function \n",
    "#\n",
    "\n",
    "def rosenbrock(x):\n",
    "    val = TODO\n",
    "    grad = TODO\n",
    "    return val, grad\n",
    "\n",
    "#\n",
    "# And test its gradient\n",
    "#\n",
    "check_gradient(rosenbrock, np.array([0.,0.]), delta=1e-8)\n",
    "check_gradient(rosenbrock, np.array([1.0,1.0]), delta=1e-8)\n",
    "\n",
    "#\n",
    "# Find the optimum\n",
    "#\n",
    "\n",
    "X0= (0.,2.)\n",
    "Xopt, Xhist = GD(rosenbrock, X0, alpha=TODO, stop_tolerance=1e-10)\n",
    "Xhist = np.array(Xhist)\n",
    "\n",
    "print \"Found optimum at %s in %d steps (true minimum is at [1,1])\" % (Xopt, len(Xhist))\n",
    "\n",
    "#\n",
    "# Plot the value over iterations\n",
    "#\n",
    "\n",
    "#TODO\n",
    "\n",
    "#\n",
    "# Make a contour plot\n",
    "#\n",
    "# you may want to use functions: meshgrid, contour\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Use scipy.optimize.fmin_l_bfgs_b\n",
    "# Again plot the path on the coutnour plot\n",
    "#\n",
    "# Hint: to save the points you can use the callback argument!\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print 'Features: ', iris.feature_names\n",
    "print 'Targets: ', iris.target_names\n",
    "petal_length = iris.data[:,iris.feature_names.index('petal length (cm)')]\n",
    "petal_width = iris.data[:, iris.feature_names.index('petal width (cm)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract the petal_length and petal_width of versicolors and virginicas\n",
    "\n",
    "IrisX = np.vstack([np.ones_like(petal_length), petal_length, petal_width])\n",
    "IrisX = IrisX[:, iris.target!=0]\n",
    "\n",
    "# Set versicolor=0 and virginia=1\n",
    "IrisY = (iris.target[iris.target!=0]-1).reshape(1,-1).astype(np.float64)\n",
    "\n",
    "scatter(IrisX[1,:], IrisX[2,:], c=IrisY.ravel(), cmap='spring')\n",
    "xlabel('petal_length')\n",
    "ylabel('petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionCost(object):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    #note: this creates a Pyton callable - i.e. an abject that can be called as a function\n",
    "    def __call__(self, Theta):\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "        \n",
    "        #reshape Theta\n",
    "        ThetaR = Theta.reshape(X.shape[0],1)\n",
    "        \n",
    "        #\n",
    "        # Fill in negative log likelihood computation\n",
    "        # and gradient computation\n",
    "        #\n",
    "        # Properly implemented, this takes about 3 lines of code!\n",
    "        #\n",
    "        \n",
    "        nll = TODO\n",
    "        \n",
    "        grad = TODO\n",
    "        \n",
    "        #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
    "        return nll, grad.reshape(Theta.shape)\n",
    "\n",
    "iris_log_reg = LogisticRegressionCost(IrisX, IrisY)\n",
    "\n",
    "Theta0 = np.zeros((3))\n",
    "check_gradient(iris_log_reg, Theta0)\n",
    "\n",
    "#\n",
    "# Maybe check the gradients at a few other points too?\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "# Call a solver\n",
    "#\n",
    "\n",
    "ThetaOpt = GD(iris_log_reg, Theta0, alpha=TODO)[0]\n",
    "\n",
    "#\n",
    "# TODO: also tru f_min_lbfsgb??\n",
    "#\n",
    "\n",
    "check_gradient(iris_log_reg, ThetaOpt)\n",
    "\n",
    "\n",
    "#\n",
    "# Now plot the found separation line \n",
    "# \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
