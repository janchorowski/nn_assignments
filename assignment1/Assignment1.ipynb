{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Submission deadline:**\n",
    "* **Problems 1-4: last lab session before or on Thursday, 9.3.17**\n",
    "* **Problems 5-6: last lab session before or on Thursday, 16.3.17**\n",
    "\n",
    "**Points: 10 + 4 bonus points**\n",
    "\n",
    "Please note: some of the assignments are tedious or boring if you are already a NumPy ninja. The bonus problems were designed to give you a more satisfying alternative.\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions. Whenever possible, add your solutions to the notebook. \n",
    "\n",
    "Please email us about any problems with it - we will try to correct them quickly. Also, please do not hesitate to use GitHub’s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard IPython notebook imports\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0 [0p]\n",
    "\n",
    "Get familiar with our [lab guide](https://docs.google.com/viewer?a=v&pid=sites&srcid=Y3MudW5pLndyb2MucGx8amNofGd4OjM0ZTU2MjA4M2NlOGIwNmE). It is not obligatory to use the labs remotely and most of the time you will do just fine with your own machine. However, we recommend using labs for the most computationally-intensive tasks such as the final projects.\n",
    "\n",
    "Make sure you know how to:\n",
    "* load up-to-date Anaconda Python distribution with `/pio/os/anaconda/set-env.sh` (confirm with `python -V`)\n",
    "<br/>\n",
    "(also: look into this `nn_assignments`' `set-env.sh` and see that it loads `Anaconda` for you),\n",
    "* connect to labs 110/137 from *outside* of the faculty,\n",
    "* copy a file over an ssh tunnel or connect remotely to a Jupyter Notebook,\n",
    "* use `GNU screen` and/or `nohup`,\n",
    "* leave a process running, disconnect, connect again and verify it is still running.\n",
    "\n",
    "In case of problems, ask for help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 [2p]\n",
    "\n",
    "First, get familiar with Python at https://docs.python.org/2/tutorial/. Then, get\n",
    "to know the capabilities of NumPy, the prime numerical library of Python http://www.numpy.org/, for instance with the tutorial at http://wiki.scipy.org/Tentative_NumPy_Tutorial.\n",
    "\n",
    "You might also need:\n",
    "  1. another intro to NumPy,\n",
    "http://people.duke.edu/~ccc14/pcfb/numerics.html\n",
    "  2. a better interactive shell for Python,\n",
    "http://ipython.org/\n",
    "  3. access to IPython through an ordinary web browser,\n",
    "http://ipython.org/notebook.html\n",
    "  4. a plotting library for Python.\n",
    "http://matplotlib.org/\n",
    "\n",
    "**a) Declare variables:**\n",
    "1. $a=10$,\n",
    "2. $b=2.5\\times 10^{23}$,\n",
    "3. $c=2+3i$, where $i$ is an imaginary unit,\n",
    "4. $d=e^{i2\\pi/3}$, where $i$ is an imaginary unit, $e$ is the Euler's number (use `exp`, `pi`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the declarations\n",
    "a = \n",
    "b = \n",
    "c = \n",
    "d = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Declare vectors:**\n",
    "1. $aVec=\\begin{bmatrix} 3.14 & 15 & 9 & 26 \\end{bmatrix}$,\n",
    "2. $bVec=\\begin{bmatrix} 2.71 & 8 & 28 & 182 \\end{bmatrix}^\\intercal$ (column vector),\n",
    "3. $cVec=\\begin{bmatrix} 5 & 4.8 & \\cdots & -4.8 & -5 \\end{bmatrix}$ (vector of numbers from $5$ to $-5$ decreasing by $0.2$),\n",
    "4. $dVec=\\begin{bmatrix} 10^0 & 10^{0.01} & \\cdots & 10^{0.99} & 10^1 \\end{bmatrix}$ (logarithmically spaced numbers from 1 to 10, use `logspace` and make sure, that the result has correct length!),\n",
    "5. $eVec=Hello$ ($eVec$ is a string of characters, thus a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aVec = \n",
    "bVec = \n",
    "cVec = \n",
    "dVec = \n",
    "eVec = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Declare matrices:**\n",
    "1. $aMat=\\begin{bmatrix}\n",
    "                    2      & \\cdots & 2 \\\\\n",
    "                    \\vdots & \\ddots & \\vdots \\\\\n",
    "                    2      & \\cdots & 2\n",
    "                \\end{bmatrix}$,\n",
    "<br/>\n",
    "matrix $9\\times 9$ filled with 2s (use `ones` or `zeros`),\n",
    "2. $bMat=\\begin{bmatrix}\n",
    "                    1      & 0      & \\cdots &        & 0      \\\\\n",
    "                    0      & \\ddots & 0      &        & 0      \\\\\n",
    "                    \\vdots & 0      & 5      & 0      & \\vdots \\\\\n",
    "                           &        & 0      & \\ddots & 0      \\\\\n",
    "                    0      &        & \\cdots & 0      & 1\n",
    "                \\end{bmatrix}$,\n",
    "<br/>\n",
    "matrix $9\\times 9$ filled with zeros, with $\\begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 4 & 3 & 2 & 1 \\end{bmatrix}$ on its diagonal (use `zeros`, `diag`),\n",
    "3. $cMat=\\begin{bmatrix}\n",
    "                    1      & 11     & \\cdots & 91     \\\\\n",
    "                    2      & 12     & \\ddots & 92     \\\\\n",
    "                    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                    10     & 20     & \\cdots & 100\n",
    "                \\end{bmatrix}$,\n",
    "<br/>\n",
    "matrix $10\\times 10$, columns of which form the vector $1:100$ (use `reshape`),\n",
    "4. $dMat=\\begin{bmatrix}\n",
    "                    NaN & NaN & NaN & NaN \\\\\n",
    "                    NaN & NaN & NaN & NaN \\\\\n",
    "                    NaN & NaN & NaN & NaN\n",
    "                \\end{bmatrix}$,\n",
    "<br/>\n",
    "matrix $3\\times 4$ filled with `NaN`s (use... `NaN`),\n",
    "5. $eMat=\\begin{bmatrix}\n",
    "                    13  & -1  & 5  \\\\\n",
    "                    -22 & 10  & -87\n",
    "                \\end{bmatrix}$,\n",
    "<br/>\n",
    "6. $fMat$ filled with random natural numbers from $[-3,3]$ (use `rand` and `floor` or `ceil`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aMat = \n",
    "bMat = \n",
    "cMat = \n",
    "dMat = \n",
    "eMat = \n",
    "fMat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** d) Declare a multiplication table ** as a $10\\times 10$ matrix `mulMat`. Use matrix/vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mulMat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** e) Compute elemwise using values from b).**\n",
    "For instance, the first element of $xVec[0]$ should be equal to\n",
    "\n",
    "\\begin{equation}\n",
    "1/(\\sqrt{2\\pi2.5^2}) e^{-cVec[0]^2 / (2\\cdot\\pi 2.5^2)}.\n",
    "\\end{equation}\n",
    "\n",
    "1. $xVec=1/(\\sqrt{2\\pi2.5^2}) e^{-cVec^2 / (2\\cdot\\pi 2.5^2)}$\n",
    "2. $yVec=\\sqrt{(aVec^\\intercal)^2 + bVec^2}$\n",
    "3. $zVec=\\log_{10}(1/dVec)$, using `log10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xVec = \n",
    "yVec = \n",
    "zVec = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** f) Compute with matrix/vector operations using values from c).**\n",
    "1. $xMat=(aVec\\cdot bVec)aMat^2$,\n",
    "2. $yMat=bVec\\cdot aVec$\n",
    "<br/>\n",
    "(remember, that matrix multiplication is not commutative),\n",
    "4. $zMat=\\lvert cMat\\rvert (aMat\\cdot bMat)^\\intercal$, where $\\lvert A\\rvert$ denotes determinant of $A$ (use `det`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xMat = \n",
    "yMat = \n",
    "zMat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** g) Declare `ismagic(A)` function ** which checks if matrix $A$ is a [magic square](https://en.wikipedia.org/wiki/Magic_square) and returns a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ismagic(A):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "The following excerpt of code loads the data describing iris flowers\n",
    "and shows relations between their length and petal width for three\n",
    "species (namely: setosa, versicolor, virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pylab - loads numerical and graphical libraries into the IPython notebook\n",
    "%pylab inline\n",
    "\n",
    "# sklearn is a large collection of machine learning algorithms\n",
    "# here we’ll use it only for the built-in iris dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print('Features: ', iris.feature_names)\n",
    "print('Targets: ', iris.target_names)\n",
    "\n",
    "petal_length = iris.data[:,iris.feature_names.index('petal length (cm)')]\n",
    "petal_width = iris.data[:, iris.feature_names.index('petal width (cm)')]\n",
    "\n",
    "for target in set(iris.target):\n",
    "    example_ids = target==iris.target\n",
    "    scatter(petal_length[example_ids], petal_width[example_ids],\n",
    "            label=iris.target_names[target], color='bgr'[target],\n",
    "            marker='x', alpha=0.7)\n",
    "unknown = np.array([\n",
    "                    [1.5, 0.3],\n",
    "                    [4.5, 1.2],\n",
    "                    [5.5, 2.3],\n",
    "                    [5.1, 1.7]\n",
    "                   ])\n",
    "scatter(unknown[:,0], unknown[:,1], marker='v', color='gray', s=50, label='??')\n",
    "xlabel('petal length (cm)')\n",
    "ylabel('petal width (cm)')\n",
    "grid(True)\n",
    "legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these two features, it is easy to distinguish iris setosa from the two remaining species. Yet iris versicolor and virginica remain mixed together. \n",
    "\n",
    "Looking closely at the plot, we might estimate the spieces of the selected unknown irises (gray triangles). For three of them the answer seems obvious – they belong in uniformly-colored areas covered by one spieces only. Yet unknown iris flower in (5.1, 1.7) is troublesome – it lays on the boundary of versicolor and virginica clusters. We can assume, that its species is the one of the closest one to it, coming from the training set (and so having a label). \n",
    "\n",
    "K-Nearest Neighbors method (http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) solves the classification problem, i.e. sets class labels (species in case of irises) of a previously unseen sample by choosing the most common class among the top k neighbors of the sample in question (for instance according to the Euclidean distance). Thus, the k-Nearest Neighbors algorithm works as follows. For each unlabeled sample x:\n",
    "1. Find k nearest neighbors among the labeled samples.\n",
    "2. Set the most common label among them as label of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 [2p]\n",
    "\n",
    "1.  **[1p]** Load the iris data (in Python it’s built-in into machine learning libraries, use sklearn.datasets.load_iris), the data is also available on-line at https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "2.  **[1p]** Irises are described with 4 attributes: petal and sepal widths and lengths. We often plot such data as matrices depicting relationships between pairs of attributes (the diagonal of which holds an ordinary histogram). Write code making a plot like the one below. Please pay attention to the details: make a proper legend and correctly label the axes.\n",
    "\n",
    "<img src=\"iris4x4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 [2p]\n",
    "\n",
    "Implement the k-Nearest Neighbors algorithm. Try to\n",
    "take advantage of matrix calculus rather than using for loops.\n",
    "\n",
    "**Tip:** What is computed by \\begin{equation} \\sqrt{(X - Y)^T (X - Y)} \\end{equation} when both X and Y are vectors?\n",
    "\n",
    "**Tip:** Try to use broadcasting (NumPy: http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) and built-ins sort, numpy.sort, numpy.argsort (sorting), scipy.stats.mode (choosing the most common element of the set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 [2p]\n",
    "Consider the following experiment:\n",
    "1. We scramble the data and split it into two parts - training set (66.6% of all samples) and test set (33.4%).\n",
    "2. Based on the training set, we use the k-NN algorithm to predict the labels on the test set.\n",
    "3. We then check the number of errors and write it down.\n",
    "\n",
    "Do this 500 times for k ∈ {1, 3, 5, ..., 19}. Plot a function of the average number of errors\n",
    "as the function of k. It should be similar to the one below.\n",
    "\n",
    "<img src=\"knn.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 [2p] \n",
    "\n",
    "Apply the K-Nearest Neighbors (K-NN) algorithm to the MNIST and CIFAR10 datasets. \n",
    "\n",
    "The MNIST (http://yann.lecun.com/exdb/mnist/) dataset consists of normalized (centered and stretched) scans of hand-written digits. Specifically, each element of the dataset is a 28 × 28 grayscale image, thus having 764 8-bit pixels. \n",
    "\n",
    "The CIFAR10 (http://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of small, 32 by 32 pixels, RGB images belonging to 10 categories.\n",
    "\n",
    "1. **[1p]** Download and load the MNIST and CIFAR10 datasets. For both datasets, display a few objects from each of the classes, paying attention to aesthetics and clarity of your presentation. **Note:** the datasets are available on Lab computers. Please use the code below to get started.\n",
    "\n",
    "2. **[2p]** Apply a k-NN classifier to the MNIST and CIFAR10 datasets. First, divide the training set into two parts, which we will call training and validation. On MNIST use the first 50000 samples for training and the last 10000 for validation. On CIFAR10, use 40000 to train and 10000 for validation. Then find the optimal number of neighbors by assessing the accuracy on the validation set. You do not need to repeat this experiment multiple times. Finally, compute the accuracy on the test set obtained with the best previously chosen number of neighbors. On MNIST you should get about 3% errors, while on CIFAR10 you should get about 70% errors. Why CIFAR10 is harder than MNIST? Pick a few mislabeled samples from the test dataset and plot them along with the correct ones. \n",
    "\n",
    "**Note:**\n",
    "  * MNIST and CIFAR10 are much larger than the Iris dataset. A good implementation needs about XXX minutes on Lab computers. Please optimize your algorithm:\n",
    "  * Compute the distances only once, then test for different values of k.\n",
    "  * Use vectorized expressions to compute the distance. It is possible to compute all distances between the training and testing points in one expression. Hint: think about the vectorized expression \\begin{equation}(X - Y)^T (X - Y).\\end{equation}\n",
    "  * You can use single precision numbers in computation.\n",
    "  * If your code is taking a long time o execute, please save its results before the lab session.\n",
    "\n",
    "**Note:** in NumPy, matrices have its own data type (dtype), which is retained during\n",
    "calculations. Please pay attention, i.e. do not subtract values of data types not\n",
    "having the sign bit, do not divide integers, etc. Results of such operations won’t be\n",
    "automatically casted to types having the required precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading MNIST data\n",
    "\n",
    "from fuel.datasets.mnist import MNIST\n",
    "from common.plotting import plot_mat\n",
    "\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "\n",
    "print(\"We have %d training, %d validation, and %d test examples\" % (\n",
    "    mnist_train.num_examples, mnist_validation.num_examples, mnist_test.num_examples))\n",
    "print(\"The examples are pairs of %s:\" % (mnist_train.sources,))\n",
    "for i, source in enumerate(mnist_train.sources):\n",
    "    labels = mnist_train.axis_labels[source]\n",
    "    print('The source #%d named \"%s\" is a %dd array with axis: %s' % (\n",
    "        i, source, len(labels), labels))\n",
    "\n",
    "#Note that for larger datasets that are loaded into mameory the data_sources field may not exist!\n",
    "plot_mat(mnist_train.data_sources[0][:20], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: plot an example of each class on MNIST and on CIFAR\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract the data matrices\n",
    "\n",
    "mnist_train_X = (mnist_train.data_sources[0].reshape(mnist_train.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_train_Y = mnist_train.data_sources[1].ravel()\n",
    "\n",
    "mnist_valid_X = (mnist_validation.data_sources[0].reshape(mnist_validation.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_valid_Y = mnist_validation.data_sources[1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def KNN(train_X, train_Y, test_X, test_Y, ks, batch_size=200):\n",
    "    \"\"\"\n",
    "    Compute error rate for various \n",
    "    \"\"\"\n",
    "    errs = np.zeros((len(ks),))\n",
    "    for i in xrange(0,test_Y.shape[0], batch_size):\n",
    "        batch_X = test_X[i:i + batch_size]\n",
    "        batch_Y = test_Y[i:i + batch_size]\n",
    "        print(\"Examples %d:%d Computing distances... \" %(i,i+batch_size), end='')\n",
    "        \n",
    "        #\n",
    "        # TODO: fill in an efficient distance matrix computation \n",
    "        #\n",
    "        dists = TODO\n",
    "        \n",
    "        print(\"Sorting... \", end='')\n",
    "        closest = np.argsort(dists,0)\n",
    "\n",
    "        print(\"Computing errors...\")\n",
    "        targets = train_Y[closest]\n",
    "        \n",
    "        for ki,k in enumerate(ks):\n",
    "            predictions, unused_counts = mode(targets[:k,:], axis=0)\n",
    "            predictions = predictions.ravel()\n",
    "            #\n",
    "            # TODO: fill in error count computation\n",
    "            #\n",
    "            errs[ki] += TODO\n",
    "        \n",
    "    errs /= test_Y.shape    \n",
    "    return np.vstack((ks, errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now find the best k on the validation set\n",
    "\n",
    "mnist_validation_errs = KNN(mnist_train_X, mnist_train_Y, mnist_valid_X, mnist_valid_Y, [1,3,5,7,9])\n",
    "plot(mnist_validation_errs[0,:], mnist_validation_errs[1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now use the best k to compute the test error\n",
    "\n",
    "best_K =  TODO\n",
    "\n",
    "mnist_full_train = MNIST((\"train\",), )\n",
    "\n",
    "mnist_full_train_X = (mnist_full_train.data_sources[0].reshape(mnist_full_train.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_full_train_Y = mnist_full_train.data_sources[1].ravel()\n",
    "\n",
    "mnist_test_X = (mnist_test.data_sources[0].reshape(mnist_test.num_examples, -1) / 255.0).astype(np.single)\n",
    "mnist_test_Y = mnist_test.data_sources[1].ravel()\n",
    "\n",
    "mnist_test_errs = KNN(mnist_full_train_X, mnist_full_train_Y, mnist_test_X, mnist_test_Y, [best_K])\n",
    "print(\"When k=%d the test error rate is %.1f%%\" % (mnist_test_errs[0,0], mnist_test_errs[1,0]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now repeat the k-NN training for CIFAR10\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality sensitive hashing\n",
    "\n",
    "Problem 5 was about speeding up the inference using loops implicitly present in matrix multiplication instead of explicit loops in Python. In this problem, we will explore a strategy to truly reduce the total number of computations required to find nearest neighbors without sacrificing too much accuracy.\n",
    "\n",
    "To speed up nearest neighbor search we will employ *Locality Sensitive Hashing (LSH)* functions. For a given distance metric, the locality sensitive hash should put items that are similar into the same bucket. Notice that this is essentially a design choice opposite to traditional cryptographic hash functions that should amplify the difference of similar inputs (typically we want that small perturbations of data result in large changes to the hash value).\n",
    "\n",
    "One of the simplest implementations of LSH approximates the cosine distance. Let $x\\in \\mathbb{R}^N$ and $y\\in \\mathbb{R}^N$ be two vectors. Their cosine distance is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    d_\\text{cos}(x,y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} = \\cos\\left(\\theta(x,y)\\right),\n",
    "\\end{equation}\n",
    "where $\\theta(x,y)$ is the unsigned angle between $x$ and $y$.\n",
    "\n",
    "We will construct a family $H$ of hash functions that are an LSH for angle distances (an approximation to cosine distance). Assume $p\\in \\mathbb{R}^N$ is a random vector (components are sampled from the normal distribution) of length 1. Then define the hash function $h(x) = \\text{sgn}(x\\cdot p)$, where $\\text{sgn()}$ is the sign function. It can be proven that:\n",
    "\n",
    "\\begin{equation}\n",
    "    p_{h\\in H}[h(x)=h(y)] = 1 - \\frac{\\theta(x,y)}{\\pi}.\n",
    "\\end{equation}\n",
    "\n",
    "The equation means that the probability of a hash collision grows as the the angle between two vectors gets smaller. Therefore, vectors that are close according to the cosine distance will be put with high probability into the same bin (we use the fact that for small $\\theta$ we can approximate $\\cos(\\theta) = 1 - \\theta/\\pi$.\n",
    "\n",
    "We will say that a family of randomly chosen hash functions $H$ is $(d_1, d_2, p_1, p_2)$-sensitive with respect to a distance metric $d$ if for any $x$ and $y$:\n",
    "1. If $d(x,y) \\leq d_1$ then $p_{h\\in H}[h(x)=h(y)] \\geq p_1$.\n",
    "2. If $d(x,y) \\geq d_2$ then $p_{h\\in H}[h(x)=h(y)] \\leq p_2$.\n",
    "\n",
    "For example, our family of randomly chosen hyperplanes is $(d_1, d_2, (1-d_1)/\\pi, (1-d_2)/\\pi)$-sensitive.\n",
    "\n",
    "Ideally, vectors should be placed into the same bin with a high probability if their distance is smaller than a threshold, and with a low probability if their distance is larger that the threshold. By combining hashing functions we can get closer to this ideal sensitivity.\n",
    "\n",
    "Given a family of hash functions $H$ with sensitivity $(d_1, d_3, p_1, p_2)$ we can construct a new family $H'$ by combining $r$ functions from $H$:\n",
    "1. AND: let $h=[h_1, h_2, \\ldots, h_r] \\in H'$ and $h(x)=h(y)$ if and only if $\\forall_i h_i(x)=h_i(y)$. Then $H'$ is $(d_1, d_2, (p_1)^r, (p_2)^r)$-sensitive.\n",
    "2. OR: let $h=[h_1, h_2, \\ldots, h_r] \\in H'$ and $h(x)=h(y)$ if and only if $\\exists_i h_i(x)=h_i(y)$. Then $H'$ is $(d_1, d_2, 1-(1-p_1)^r, 1-(1-p_2)^r)$-sensitive.\n",
    "\n",
    "AND makes all probabilities shrink, but properly choosing $r$ we can make the lower probability approach 0 while the higher does not. Conversely, OR makes all probabilities grow, we can make the upper probability approach 1 while the lower does not.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 6 [2-4p bonus] \n",
    "\n",
    "1. **[1bp]** **Note:** you can show sketches of proofs for this assignment.\n",
    "    1. Show that angle between vectors is a metric (https://en.wikipedia.org/wiki/Metric_(mathematics)).\n",
    "    \n",
    "    2. Show that $p_{h\\in H}[h(x)=h(y)] = 1 - \\frac{\\theta(x,y)}{\\pi}$ for $h$ computed using a randomly chosen hyperplane.\n",
    "\n",
    "    3. Show the properties of either AND or OR boosting of LSH.\n",
    "\n",
    "3. **[1-3bp]** Reimplement k-Nearest Neighbors for MNIST classification using the cosine distance instead of the Euclidean distance. Choose a sensible value of $k$. Use Locality Sensitive Hashing to achieve an error rate no greater than $150\\%$ of the original error rate with at least a $90\\%$ speedup (i.e., by considering on average at most 5000 training samples per query image). For a few settings plot the speedup-vs-accuracy relation.\n",
    "\n",
    "  **Note:** points will be awarded based on ingenuity of your solution. Feel free to explore your own ideas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
